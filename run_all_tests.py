#!/usr/bin/env python3
"""
ğŸ§ª EJECUTAR TODOS LOS TESTS DEL SDK
"""

import asyncio
import subprocess
import sys
from pathlib import Path

def run_command(command, description):
    """Ejecutar comando y mostrar resultado"""
    print(f"\nğŸ” {description}")
    print("="*50)
    
    try:
        result = subprocess.run(
            command, 
            shell=True, 
            capture_output=True, 
            text=True,
            timeout=300  # 5 minutos timeout
        )
        
        if result.returncode == 0:
            print(f"âœ… {description}: Ã‰XITO")
            if result.stdout:
                print("ğŸ“Š Resultado:")
                print(result.stdout[-500:])  # Ãšltimas 500 lÃ­neas
        else:
            print(f"âŒ {description}: FALLÃ“")
            if result.stderr:
                print("ğŸš¨ Error:")
                print(result.stderr[-500:])
                
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        print(f"â° {description}: TIMEOUT (>5 min)")
        return False
    except Exception as e:
        print(f"ğŸ’¥ {description}: ERROR - {e}")
        return False

def main():
    """Ejecutar todos los tests"""
    print("ğŸš€ EJECUTANDO TODOS LOS TESTS DEL SDK")
    print("="*60)
    
    tests = [
        # Tests bÃ¡sicos
        ("python test_authentication_system.py", "Sistema de AutenticaciÃ³n"),
        ("python tests/integration/test_auth_integration.py", "IntegraciÃ³n de AutenticaciÃ³n"),
        ("python tests/integration/test_full_system.py", "IntegraciÃ³n Completa del Sistema"),
        
        # Tests de componentes especÃ­ficos
        ("python test_web_dashboard_complete.py", "Dashboard Web Completo"),
        ("python test_template_management_complete.py", "Sistema de Templates"),
        ("python test_log_management_complete.py", "Sistema de Logs"),
        ("python test_configuration_api_complete.py", "API de ConfiguraciÃ³n"),
        
        # Tests de performance
        ("python tests/performance/test_performance_benchmarks.py", "Benchmarks de Performance"),
        
        # Tests finales
        ("python test_sdk_final_validation.py", "ValidaciÃ³n Final del SDK"),
    ]
    
    results = {}
    total_tests = len(tests)
    passed_tests = 0
    
    for command, description in tests:
        success = run_command(command, description)
        results[description] = "âœ… PASSED" if success else "âŒ FAILED"
        if success:
            passed_tests += 1
    
    # Resumen final
    print("\n" + "="*60)
    print("ğŸ“‹ RESUMEN FINAL DE TESTS")
    print("="*60)
    
    for test_name, result in results.items():
        print(f"{test_name}: {result}")
    
    success_rate = (passed_tests / total_tests) * 100
    print(f"\nğŸ“Š RESULTADOS FINALES:")
    print(f"   âœ… Tests Exitosos: {passed_tests}/{total_tests}")
    print(f"   ğŸ“ˆ Tasa de Ã‰xito: {success_rate:.1f}%")
    
    if success_rate >= 80:
        print(f"\nğŸ‰ Â¡EXCELENTE! El SDK estÃ¡ funcionando correctamente")
        print(f"ğŸš€ Listo para usar en producciÃ³n")
    elif success_rate >= 60:
        print(f"\nâš ï¸  El SDK funciona bien pero tiene algunas Ã¡reas de mejora")
        print(f"ğŸ”§ Revisar los tests que fallaron")
    else:
        print(f"\nğŸš¨ El SDK necesita atenciÃ³n - varios tests fallaron")
        print(f"ğŸ› ï¸  Revisar y corregir los problemas identificados")

if __name__ == "__main__":
    main()